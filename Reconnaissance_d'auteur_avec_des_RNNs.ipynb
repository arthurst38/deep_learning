{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reconnaissance d'auteur avec des RNNs.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lTlp1Ju4ky4w",
        "qG4Y65KaLAuX",
        "anrc7R2wLvHm",
        "JbJJIMFPLxAN",
        "i1q4ZGDoMPO4",
        "h9PJFatsybrb",
        "0YyiAic6NpNq",
        "IK1l6x2SOICj",
        "7SdGK5obl_9K"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurst38/deep_learning/blob/main/Reconnaissance_d'auteur_avec_des_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmRQqS_OVa06"
      },
      "source": [
        "# Reconnaissance d'auteur avec des RNNs\n",
        "\n",
        "Dans ce TP, nous allons utiliser des RNNs pour retrouver l'auteur d'un texte.\n",
        "\n",
        "Le corpus utilisé est tiré d'œuvres littéraires classiques de littérature anglophone : 9 auteurs, 2 livres par auteurs avec un fichier par chapitre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Av-QXTvSqW"
      },
      "source": [
        "## Téléchargement des données depuis un répo git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ3hnIM_o3Cn"
      },
      "source": [
        "!git clone https://github.com/nzmonzmp/dataset-9classical-author.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs9mK-SrvVNg"
      },
      "source": [
        "## Téléchargement du modèle spacy pour l'anglais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD3v1-C5vZf_"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQgGAjFvbYG"
      },
      "source": [
        "## Import de TensorFlow et des autres librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDDBsMNGvLir"
      },
      "source": [
        "import re\n",
        "import typing\n",
        "\n",
        "import numpy\n",
        "import pathlib\n",
        "import seaborn\n",
        "import sklearn.model_selection\n",
        "import sklearn.preprocessing\n",
        "import spacy\n",
        "import tensorflow.keras as keras\n",
        "import tqdm.notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzIeJTAGviTi"
      },
      "source": [
        "## Prétraitements\n",
        "\n",
        "- Chaque phrase est un exemple\n",
        "- Sont remplacés par leur type tout mot (ou groupe de mots) étant une entité nommée de type :\n",
        "  - Personne\n",
        "  - Bâtiment\n",
        "  - Institution\n",
        "  - Lieu\n",
        "  - Date précise\n",
        "  - Monnaie\n",
        "  - Œuvre artistique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOdn9dskvf5V"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "ner_to_replace= dict(PERSON=\"person\",\n",
        "                     FAC=\"building\" , \n",
        "                     ORG=\"organization\", \n",
        "                     GPE=\"city\" ,\n",
        "                     LOC=\"lake\" , \n",
        "                     DATE=\"date\" , \n",
        "                     MONEY=\"dollar\" , \n",
        "                     WORK_OF_ART=\"painting\")\n",
        "\n",
        "\n",
        "def get_data(directory: pathlib.Path\n",
        "            ) -> typing.Tuple[typing.List[str], typing.List[str]]:\n",
        "  texts = []\n",
        "  authors = []\n",
        "  for item in tqdm.notebook.tqdm(list(directory.glob(\"*/*/*.txt\"))):\n",
        "    author = item.parent.parent.name\n",
        "    text = item.read_text(encoding=\"utf8\")\n",
        "    doc = nlp(\" \".join(text.split()))\n",
        "    for s in doc.sents:\n",
        "      seq = [(ner_to_replace[t.ent_type_]\n",
        "              if t.ent_type_ in ner_to_replace\n",
        "              else t.text)\n",
        "             for t in s\n",
        "             if t.ent_iob_ != \"I\"]\n",
        "      if not re.search(\"chapter\", seq[0], re.IGNORECASE):\n",
        "        texts.append((\" \".join(seq)))\n",
        "        authors.append(author)\n",
        "  return texts, authors\n",
        "\n",
        "texts, authors = get_data(pathlib.Path(\"dataset-9classical-author\"))\n",
        "print(len(texts), len(authors))\n",
        "\n",
        "\n",
        "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(authors)\n",
        "print(y)\n",
        "X_train_raw, X_test_raw, y_train, y_test = \\\n",
        "    sklearn.model_selection.train_test_split(texts, y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhNwXLpK-om"
      },
      "source": [
        "print(len(X_train_raw), y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-SyUp-bEVlL"
      },
      "source": [
        "## Préparation des données en séquences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwYkkIXCvqUH"
      },
      "source": [
        "### Préparation du dictionnaire et des séquences\n",
        "\n",
        "À l'aide de l'outil [`tensorflow.keras.preprocessing.text.Tokenizer`](https://keras.io/api/preprocessing/text/#tokenizer-class) :\n",
        "- Constituez un dictionnaire sur le Corpus `X_train_raw`\n",
        "- Quelle est la taille du vocabulaire ?\n",
        "- Quelle est la taille maximum, en nombre de mots, d'une phrase ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B2hIel6ktxj"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTlp1Ju4ky4w"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLCeBm86gUfa"
      },
      "source": [
        "tokenizer_obj = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X_train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2wOTyj54Yl4"
      },
      "source": [
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "max_length = max(len(s.split()) for s in X_train_raw)\n",
        "print(f\"Taille du vocabulaire : {vocab_size}\")\n",
        "print(f\"Taille de la plus grande séquence de mot : {max_length}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ZCcOI0RqRH"
      },
      "source": [
        "### Création de la matrice de séquences d'indices\n",
        "\n",
        "À l'aide des outils [`tensorflow.keras.preprocessing.text.Tokenizer`](https://keras.io/api/preprocessing/text/#tokenizer-class) et [`tensorflow.keras.preprocessing.sequence.pad_sequences`](https://keras.io/api/preprocessing/timeseries/#padsequences-function) :\n",
        "- Transformez `X_train_raw` et `X_test_raw` en séquences d'indices de mots dans un dictionnaire. (fonction ``texts_to_sequences()`` de l'objet ``Tokenizer`` instancié précédemment)\n",
        "- Effectuez l'opération de **padding** sur les séquences afin qu'elles aient une taille raisonnable pour être traitées par un GRU bidirectionnel :\n",
        "  - On utilisera `maxlen = 150` car au-delà les GRU commencent à ne plus être capable de transmettre correctement l'information.\n",
        "- Stockez les séquences obtenues dans `X_train_pad` et `X_test_pad`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuqz8JhiLAuC"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG4Y65KaLAuX"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZae4HhR46et"
      },
      "source": [
        "max_length = 150\n",
        "\n",
        "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train_raw)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test_raw)\n",
        "\n",
        "X_train_pad = keras.preprocessing.sequence.pad_sequences(\n",
        "    X_train_tokens, maxlen=max_length, truncating=\"pre\")\n",
        "X_test_pad = keras.preprocessing.sequence.pad_sequences(\n",
        "    X_test_tokens, maxlen=max_length, truncating=\"pre\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlNksjVS719M"
      },
      "source": [
        "print(f\"Forme du corpus de documents : {X_train_pad.shape}\")\n",
        "print(f\"Premier exemple : {X_train_pad[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTrg_YGAWKYl"
      },
      "source": [
        "## Modélisation\n",
        "\n",
        "Construisez un modèle avec pour caractéristiques :\n",
        "  - Une couche d'embeddings de taille 300\n",
        "  - Une couche de GRU bidirectionnels (Bidirectional est un keras layer):\n",
        "    - de taille `64`\n",
        "    - une initialisation **des** matrices de poids orthogonales\n",
        "    - un paramètre de dropout à `0.2` pour les parties forward et récurrentes\n",
        "  - Une couche de réseau de neurones à activation `softmax` qui prend en entrée la dernière sortie de la couche [`GRU`](https://keras.io/api/layers/recurrent_layers/gru/)\n",
        "  - Une fonction de perte basée sur l'**entropie croisée**\n",
        "  - L'optimiseur `adam`\n",
        "  - l'`accuracy` comme métrique d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rczSuYPRLvHL"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anrc7R2wLvHm"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4mpE2iumh9e"
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Input(shape=max_length))\n",
        "model.add(keras.layers.Embedding(vocab_size, EMBEDDING_DIM))\n",
        "model.add(keras.layers.Bidirectional(\n",
        "    keras.layers.GRU(64,\n",
        "                     kernel_initializer=\"orthogonal\",\n",
        "                     recurrent_initializer=\"orthogonal\",\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2)))\n",
        "model.add(keras.layers.Dense(9, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGIvkVXaaTco"
      },
      "source": [
        "## Apprentissage\n",
        "\n",
        "Apprenez votre modèle sur `X_train_pad` :\n",
        "- avec des batch de taille `1024`\n",
        "- pendant `10` itérations\n",
        "- en utilisant 30% de la base d'apprentissage pour validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFj0GizXLw_5"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbJJIMFPLxAN"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEzpBrJm7Mct"
      },
      "source": [
        "model.fit(X_train_pad,\n",
        "          y_train,\n",
        "          batch_size=1024,\n",
        "          epochs=10,\n",
        "          validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zromBdoPbV91"
      },
      "source": [
        "## Évaluation\n",
        "\n",
        "Évaluez les performances de votre modèle sur la base de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wAtfxwMPOY"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1q4ZGDoMPO4"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syRQojTExvA1"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(f\"Accuracy sur la base de test : {test_accuracy:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJrF99NyM1T"
      },
      "source": [
        "## Premières conclusions\n",
        "\n",
        "*Au vu des résultats, que peut-on dire de la qualité de cet apprentissage ? Justifiez.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L54W04FlNCmQ"
      },
      "source": [
        "Votre réponse ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9PJFatsybrb"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GHs_bkTytDP"
      },
      "source": [
        "On observe un phénomène de surapprentissage. train $\\approx$ 75% alors que validation et test $\\approx$ 50%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olcx_9SUk8o_"
      },
      "source": [
        "## Utilisation d'embeddings de mots pré-appris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hGtfPIdvSP_"
      },
      "source": [
        "Afin d'obtenir de meilleurs résultats, vous allez utiliser les embeddings pré-appris GloVe, plutôt que d'apprendre des embeddings spécifiques comme précédemment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkGgZEwmc5hx"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op1yfeJ58yBx"
      },
      "source": [
        "def glove_path(embedding_dim: int) -> pathlib.Path:\n",
        "  if embedding_dim in {50, 100, 200, 300}:\n",
        "    return pathlib.Path(\"glove.6B.{}d.txt\".format(embedding_dim))\n",
        "  else:\n",
        "    raise ValueError(\"embedding_dim must be in {50, 100, 200, 300}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYIBEwyfhqEe"
      },
      "source": [
        "## Réutilisation des Word Embeddings GloVe\n",
        "\n",
        "La couche Embedding de Keras peut être initialisée avec une matrice de poids où la ligne i correspond à l'embedding du mot i.\n",
        "\n",
        "Après un rapide coup d'oeil aux fichiers `glove.6B.300d.txt` :\n",
        "- Créez un layer `embedding_layer` de type [`tensorflow.keras.layers.Embedding`](https://keras.io/api/layers/core_layers/embedding/) :\n",
        "  - Initialisez-le avec les embeddings GloVe. \n",
        "  - Initialisez les mots de notre vocabulaire qui ne seraient pas dans GloVe avec le vecteur nul\n",
        "  - Utilisez le flag approprié pour empêcher le changement de ces embeddings pendant l'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOUPC-ZlNpNR"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YyiAic6NpNq"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpVnzpWhrOAJ"
      },
      "source": [
        "embedding_matrix = numpy.zeros((vocab_size, EMBEDDING_DIM))\n",
        "found = 0\n",
        "with glove_path(EMBEDDING_DIM).open() as fh:\n",
        "  for line in fh:\n",
        "    values = line.split(\" \")\n",
        "    word = values[0]\n",
        "    if word in tokenizer_obj.word_index:\n",
        "      found += 1\n",
        "      coeffs = numpy.array(values[1:], dtype=\"float32\")\n",
        "      embedding_matrix[tokenizer_obj.word_index[word]] = coeffs\n",
        "\n",
        "print(f\"Utilisation de {found} embeddings pré-entraînés sur {vocab_size} mots \"\n",
        "      \"dans le vocabulaire\")\n",
        "\n",
        "embedding_layer = keras.layers.Embedding(vocab_size,\n",
        "                                         EMBEDDING_DIM,\n",
        "                                         weights=[embedding_matrix],\n",
        "                                         input_length=max_length,\n",
        "                                         trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lzUOg8-k0m9"
      },
      "source": [
        "## Modélisation, apprentissage et évaluation\n",
        "\n",
        "- Créez un modèle identique à celui de la partie précédente, à ceci près que la couche d'embeddings est celle que l'on vient d'instancier à partir de GloVe\n",
        "- Entraînez ce modèle avec les mêmes paramètres que dans la partie précédente\n",
        "- Évaluez-le sur les données de test\n",
        "\n",
        "NB : pour obtenir de meilleurs résultats qu'avec des SVMs, un modèle est proposé en solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvnmkntmOICA"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK1l6x2SOICj"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGonw7W4tsEu"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(keras.layers.Bidirectional(\n",
        "    keras.layers.GRU(64,\n",
        "                     kernel_initializer=\"orthogonal\",\n",
        "                     recurrent_initializer=\"orthogonal\",\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2)))\n",
        "model.add(keras.layers.Dense(9, activation =\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St0sjo_ru07D"
      },
      "source": [
        "model.fit(X_train_pad,\n",
        "          y_train,\n",
        "          batch_size=1024,\n",
        "          epochs=20,\n",
        "          validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyq5RVX4xPAA"
      },
      "source": [
        "model.evaluate(X_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcUCJC_tOL6V"
      },
      "source": [
        "## Solution meilleure que des SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbXNnT7HaIM_"
      },
      "source": [
        "bidi_gru_params = dict(kernel_initializer=\"orthogonal\",\n",
        "                       recurrent_initializer=\"orthogonal\",\n",
        "                       dropout=0.2,\n",
        "                       recurrent_dropout=0.2)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(keras.layers.Bidirectional(\n",
        "    keras.layers.GRU(64, **bidi_gru_params, return_sequences=True)))\n",
        "model.add(keras.layers.Bidirectional(keras.layers.GRU(64, **bidi_gru_params)))\n",
        "model.add(keras.layers.Dense(9, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCfyGw4Oc-Pd"
      },
      "source": [
        "model.fit(X_train_pad,\n",
        "          y_train,\n",
        "          batch_size=1024,\n",
        "          epochs=20,\n",
        "          validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Bt2rG4cZ9C"
      },
      "source": [
        "model.evaluate(X_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3nDXd69l_85"
      },
      "source": [
        "## Affichage de la matrice de confusion en Test\n",
        "\n",
        "*Affichez la matrice de confusion (l'utilisation d'une heatmap est recommandée).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP_H45Ynl_8-"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdGK5obl_9K"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1H-19J2jVh"
      },
      "source": [
        "y_pred = model.predict_classes(X_test_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NGt_ROC4Eqy"
      },
      "source": [
        "conf_mat = sklearn.metrics.confusion_matrix(y_pred, y_test, normalize=\"true\")\n",
        "seaborn.heatmap(conf_mat,\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                cmap=\"rocket_r\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}