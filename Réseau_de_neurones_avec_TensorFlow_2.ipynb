{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Réseau de neurones avec TensorFlow 2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ZgfET2f2i1k6",
        "ekG0Po-JvzES",
        "XxNN3UsVzkpX",
        "ciax3GgC1OLv",
        "_u2B43p_DpWL",
        "nZkh0jbNK9-y",
        "W9fSdmj0E2kE",
        "qASSHbc5HmYi",
        "6kzt5Yn5OGkt",
        "UaLOndYYD8Tb",
        "S6_S6rkREt2o",
        "X_QA09cGNhCP",
        "eWgaETskNHCL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurst38/deep_learning/blob/main/R%C3%A9seau_de_neurones_avec_TensorFlow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwD0GzeIgiVP"
      },
      "source": [
        "# Réseau de neurones avec TensorFlow 2\n",
        "\n",
        "Dans ce TP, nous allons construire plusieurs réseaux de neurones : un simple d'abord (sans couche cachée), puis un perceptron multicouches (plusieurs couches cachées). Nous utiliserons le dataset MNIST pour tester l'apprentissage de ce réseau en conditions réelles.\n",
        "\n",
        "MNIST est le dataset « Hello world » du machine learning. Il est composé d'images de 28 $\\times$ 28 pixels en niveaux de gris. Ces images représentent des chiffres (0 à 9). Chaque image est associée à un label indiquant le caractère que l'image est sensée représenter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRyPihwbd12C"
      },
      "source": [
        "## Imports des librairies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqk3MzJScH0Z"
      },
      "source": [
        "Tensorflow et le dataset MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGafOBmwcH0d"
      },
      "source": [
        "import functools\n",
        "import itertools\n",
        "import random\n",
        "import typing\n",
        "\n",
        "import numpy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4YPqEhvc-Km"
      },
      "source": [
        "`matplotlib`, `numpy` et `seaborn` sont des librairies de base en machine learning avec Python que nous utiliserons ici pour la visualisation et les opérations simples sur des matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHe35A8l0BjF"
      },
      "source": [
        "## Récupération des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhR2FEA70EPS"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721sOJgoeQvm"
      },
      "source": [
        "## Regardons les données\n",
        "\n",
        "Utilisez la cellule suivante pour manipuler l'objet 'mnist'. N'hésitez pas à utiliser la complétion automatique pour parcourir ses différents attributs et méthodes.\n",
        "\n",
        "Afin de faire du machine learning, on cherche des ensembles de train et de test et à afficher quelques exemples avec leurs labels associés.\n",
        "\n",
        "Vous essayerez de répondre aux questions suivantes :\n",
        "- Combien y a-t-il d'images au total dans ce dataset ?\n",
        "- Sous quelle forme sont stockées les images ? les labels ?\n",
        "- Les classes sont-elles équilibrées ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZ1lg9gfE5E"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgfET2f2i1k6"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMdsZhE_h21w"
      },
      "source": [
        "example = X_train[0]\n",
        "example_label = y_train[0]\n",
        "\n",
        "print(f\"Format des exemples : {X_train.shape}\")\n",
        "print(f\"Format des labels : {y_train.shape}\")\n",
        "\n",
        "plt.imshow(example, cmap=\"gray_r\")\n",
        "plt.title(f\"Premier exemple du dataset ({example_label})\")\n",
        "plt.show()\n",
        "\n",
        "seaborn.countplot(x=y_train)\n",
        "plt.title(\"Décompte des différentes classes (chiffres)\")\n",
        "plt.ylabel(\"Décompte\")\n",
        "plt.xlabel(\"Chiffre\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23mddZFri7Ip"
      },
      "source": [
        "## Affichage d'exemples\n",
        "\n",
        "Affichez 25 exemples, tirés au hasard dans la base de train. Vous n'oublierez pas d'afficher le label correspondant sous une forme facile à lire.\n",
        "\n",
        "Pour cela utilisez :\n",
        "- [`numpy.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)\n",
        "- [`matplotlib.pyplot.imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html)\n",
        "- [`matplotlib.pyplot.subplots`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html) pour un affichage élégant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJ0MhFmsAxM"
      },
      "source": [
        "# Utilisation des subplots pour un affichage en grande grille 5x5\n",
        "f, ax = plt.subplots(5, 5, figsize=(15, 15))\n",
        "\n",
        "# Votre code ici\n",
        "\n",
        "# ax[2, 4].imshow(... , cmap=\"gray_r\")\n",
        "# ax[2, 4].set_title(\"Exemple n (label)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekG0Po-JvzES"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG1v8Lj8oqdC"
      },
      "source": [
        "# Création de la grille de sous-plots. On donne l'argument figsize pour agrandir\n",
        "# la taille de la figure qui est petite par défaut\n",
        "f, ax = plt.subplots(5, 5, figsize=(15, 15))\n",
        "\n",
        "# On choisit 25 indices au hasard, sans replacement (on ne veut pas afficher la\n",
        "# même image deux fois)\n",
        "random_indexes = numpy.random.choice(X_train.shape[0],\n",
        "                                     size=(5, 5),\n",
        "                                     replace=False)\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    img_index = random_indexes[i, j]\n",
        "    image = X_train[img_index]\n",
        "    label = y_train[img_index]\n",
        "\n",
        "    # Affichage avec matplotlib et sa fonction imshow, très pratique en vision par\n",
        "    # ordinateur\n",
        "    ax[i, j].imshow(image, cmap='gray_r')\n",
        "    ax[i, j].set_title(f\"Exemple {img_index} ({label})\")\n",
        "    ax[i, j].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFXPdn6y5dL"
      },
      "source": [
        "## Travail des données\n",
        "\n",
        "Les données telles quelles sont peu manipulables : pour rendre leur utilisation plus facile dans un réseau de neurones, on peut les faire passer de leurs dimensions originales `(batch, 28, 28)` à `(batch, 28²)`. On peut aussi remarquer que leur type est `uint8` : nous allons utiliser des poids pour nos réseaux de neurones qui sont incompatibles (`float32`). On peut donc d'ores et déjà convertir les type de ces données vers `float32`.\n",
        "\n",
        "De plus, les réseaux de neurones (comme leur version la plus simple, la régression linéaire) travaillent mieux sur des valeurs proches de 0.\n",
        "\n",
        "- *Passez la forme de `X_train` et `X_test` de (batch, 28, 28) à (batch, 28²)*\n",
        "- *Convertissez les tableaux numpy de type `uint8` en tenseurs TensorFlow de type `float32`*\n",
        "- *Centrez les tenseurs sur 0 et normalisez par l'écart type*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEfhzmeWzjbO"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxNN3UsVzkpX"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igG-letOzmGC"
      },
      "source": [
        "# Appel à reshape et astype de numpy. -1 signifie pour reshape de « remplir » la\n",
        "# dimension avec tous les éléments restants\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).astype(\"float32\")\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).astype(\"float32\")\n",
        "\n",
        "# Calcul de la moyenne et de l'écart type pour pouvoir normaliser\n",
        "X_mean = X_train.mean(axis=0)\n",
        "X_std = X_train.std(axis=0)\n",
        "\n",
        "# Certaines colonnes sont constantes. On pourrait les supprimer. Ici, on décide\n",
        "# plutôt de les laisser à 0 (il faut en tout cas faire quelque chose sinon on\n",
        "# divise par 0 en normalisant)\n",
        "X_std[X_std == 0] = 1\n",
        "\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = tf.constant(X_train)\n",
        "X_test = tf.constant(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilbaw80zv5dV"
      },
      "source": [
        "## Construction d'un réseau de neurones sans couche cachée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2BH5L8A1OKx"
      },
      "source": [
        "### Variables du modèle\n",
        "\n",
        "Dans un réseau de neurones sans couche cachée, $y = \\sigma(XW +b)$, où $\\sigma$ est une fonction adaptée au problème. Ici, on choisira la fonction softmax, étant donné que nous nous attaquons à un problème de classification multi-classes.\n",
        "\n",
        "*Complétez la fonction `create_weights` pour initialiser `W` et `b` comme [variables TensorFlow](https://www.tensorflow.org/guide/variable) avec la fonction [`tensorflow.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afO6HQtH1OLH"
      },
      "source": [
        "def create_weights(n_inputs: int,\n",
        "                   n_outputs: int\n",
        "                  ) -> typing.Tuple[tf.Variable, tf.Variable]:\n",
        "  W = None  # Votre code ici\n",
        "  b = None  # Votre code ici\n",
        "  return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciax3GgC1OLv"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZyPrGn1Ekj"
      },
      "source": [
        "def create_simple_nn_weights(n_inputs: int,\n",
        "                             n_outputs: int\n",
        "                            ) -> typing.Tuple[tf.Variable, tf.Variable]:\n",
        "  return (tf.Variable(tf.random.normal((n_inputs, n_outputs)), name=\"W\"),\n",
        "          tf.Variable(tf.random.normal((n_outputs,)), name=\"b\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLuRmCn51GoT"
      },
      "source": [
        "### Définition du modèle\n",
        "\n",
        "Nous allons ici préparer l'architecture de notre modèle.\n",
        "\n",
        "*Corrigez le code suivant afin que `y_pred` soit le résultat de notre réseau de neurones sans couches cachées : $\\text{softmax}(XW+b)$. On utilisera le [log du softmax](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax) plutôt que le softmax standard pour plus de stabilité numérique.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNHWP1EHprO4"
      },
      "source": [
        "class SimpleNN(tf.Module):\n",
        "  def __init__(self, n_inputs: int = 28 ** 2, n_outputs: int = 10):\n",
        "    super().__init__()\n",
        "    self.W, self.b = create_simple_nn_weights(n_inputs, n_outputs)\n",
        "\n",
        "  def __call__(self, X: tf.Tensor):\n",
        "    y_pred = X  # Votre code ici\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "SimpleNN()(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u2B43p_DpWL"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKewWhze4X-B"
      },
      "source": [
        "# Creation du modèle\n",
        "class SimpleNN(tf.Module):\n",
        "  def __init__(self, n_inputs: int = 28 ** 2, n_outputs: int = 10):\n",
        "    super().__init__()\n",
        "    self.W, self.b = create_simple_nn_weights(n_inputs, n_outputs)\n",
        "\n",
        "  def __call__(self, X: tf.Tensor):\n",
        "    return tf.nn.log_softmax(X @ self.W + self.b)\n",
        "\n",
        "\n",
        "SimpleNN()(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnWNQeVfD9eJ"
      },
      "source": [
        "### Calcul de la fonction de perte\n",
        "\n",
        "Nous allons utiliser une fonction de perte standard en classification multi-classes : l'entropie croisée. Pour cela vous pouvez faire appel à [`tensorflow.keras.losses.sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy). De plus on souhaite pouvoir interpréter directement cette valeur, il faut donc que la fonction `categorical_crossentropy` rende une valeur aggrégée (la moyenne), ce que ne fait pas la fonction de Keras.\n",
        "\n",
        "Vous pourrez utiliser pour cela [`tensorflow.math.reduce_mean`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean).\n",
        "\n",
        "*Complétez la fonction `categorical_crossentropy`.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_uAh1XFrdn"
      },
      "source": [
        "def categorical_crossentropy(X: tf.Tensor,\n",
        "                             y: tf.Tensor,\n",
        "                             model: typing.Any\n",
        "                            ) -> tf.Tensor:\n",
        "  forward = model(X)\n",
        "  loss = None  # Votre code ici\n",
        "  return loss\n",
        "\n",
        "\n",
        "categorical_crossentropy(X_train, y_train, SimpleNN())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZkh0jbNK9-y"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roj4GUNopRly"
      },
      "source": [
        "def categorical_crossentropy(X: tf.Tensor,\n",
        "                             y: tf.Tensor,\n",
        "                             model: typing.Any\n",
        "                            ) -> tf.Tensor:\n",
        "  forward = model(X)\n",
        "  cross_entropy = keras.losses.sparse_categorical_crossentropy(y,\n",
        "                                                               forward,\n",
        "                                                               from_logits=True)\n",
        "  return tf.reduce_mean(cross_entropy)\n",
        "\n",
        "\n",
        "categorical_crossentropy(X_train, y_train, SimpleNN())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcQ6k8-8Ezg9"
      },
      "source": [
        "### Métriques\n",
        "\n",
        "Pour savoir si un modèle apprend correctement, il est important de mesurer ses performances. Dans ces travaux pratiques, nous allons utiliser la performance la plus simple mais aussi une des plus informatives : l'accuracy. C'est simplement le nombre de bonnes prédictions sur le nombre total de prédictions.\n",
        "\n",
        "*Utilisez [numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) et les comparaisons de tableaux numpy pour compléter la fonction accuracy.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy44_sCzE1s9"
      },
      "source": [
        "def accuracy(y: tf.Tensor, y_pred: tf.Tensor) -> float:\n",
        "  predictions = None  # Votre code ici\n",
        "  return None\n",
        "\n",
        "\n",
        "accuracy(y_test, SimpleNN()(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9fSdmj0E2kE"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVs7-yxpE371"
      },
      "source": [
        "def accuracy(y: tf.Tensor, y_pred: tf.Tensor) -> float:\n",
        "  predictions = y_pred.numpy().argmax(axis=-1)\n",
        "  same_predictions = (predictions == y).sum()\n",
        "  return same_predictions / y.shape[0]\n",
        "\n",
        "\n",
        "accuracy(y_test, SimpleNN()(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jpYDvFwG-uQ"
      },
      "source": [
        "## Création d'un dataset TensorFlow\n",
        "\n",
        "- *Utilisez [`tensorflow.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) pour coder la fonction `create_dataset` qui crée un dataset TensorFlow à partir des tableaux NumPy `X_train` et `y_train`. Chaque exemple doit être un dictionnaire qui contient les clefs `x` et `y`.*\n",
        "- *Utilisez la fonction [`batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)* pour regrouper les exemples en batchs à l'intérieur du dataset.*\n",
        "- *Utilisez la fonction [`shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) pour que le dataset soit mélangé à chaque itération.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ecdGPFfHbBh"
      },
      "source": [
        "def create_dataset(X: tf.Tensor, y: tf.Tensor, batch_size: int = 4000\n",
        "                  ) -> tf.data.Dataset:\n",
        "  dataset = None  # Votre code ici\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qASSHbc5HmYi"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAp3DSrqHlhe"
      },
      "source": [
        "def create_dataset(X: tf.Tensor, y: tf.Tensor, batch_size: int = 4000\n",
        "                  ) -> tf.data.Dataset:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices({\"x\": X_train,\n",
        "                                                \"y\": y_train})\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.shuffle(10_000, reshuffle_each_iteration=True)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeAiK2w3LEcG"
      },
      "source": [
        "## Apprentissage\n",
        "\n",
        "Voici une boucle d'apprentissage quasiment mise en place.\n",
        "\n",
        "*Implémentez la partie manquante, qui procède à la mise à jour des paramètres pour un batch donné. Vous utiliserez pour cela [`tensorflow.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jvTf_1ZOD8m"
      },
      "source": [
        "def train(model: typing.Any,\n",
        "          X_train: tf.Tensor = X_train,\n",
        "          y_train: tf.Tensor = y_train,\n",
        "          X_test: tf.Tensor = X_test,\n",
        "          y_test: tf.Tensor = y_test,\n",
        "          epochs: int = 150,\n",
        "          batch_size: int = 4000,\n",
        "          learning_rate: float = 0.001,\n",
        "          evaluate_every: int = 10,\n",
        "          loss_function = categorical_crossentropy\n",
        "         ) -> typing.Tuple[typing.List[float], ...]:\n",
        "  accuracies = [accuracy(y_train, model(X_train))]\n",
        "  val_accuracies = [accuracy(y_test, model(X_test))]\n",
        "  losses = [loss_function(X_train, y_train, model)]\n",
        "  val_losses = [loss_function(X_test, y_test, model)]\n",
        "\n",
        "  print(f\"Métriques initiales : acc {accuracies[-1]:.4f}, \"\n",
        "        f\"val_acc {val_accuracies[-1]:.4f}, \"\n",
        "        f\"loss {losses[-1]:.4f}, \"\n",
        "        f\"val_loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "  dataset_train = create_dataset(X_train, y_train)\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    for batch in dataset_train.as_numpy_iterator():\n",
        "\n",
        "      # Votre code ici\n",
        "      pass\n",
        "\n",
        "    # Calcul et affichage de la métrique d'évaluation sur l'ensemble de\n",
        "    # validation toutes les `evaluate_every` epochs\n",
        "    if (e + 1) % evaluate_every == 0:\n",
        "      accuracies.append(accuracy(y_train, model(X_train)))\n",
        "      losses.append(loss_function(X_train, y_train, model))\n",
        "      val_accuracies.append(accuracy(y_test, model(X_test)))\n",
        "      val_losses.append(loss_function(X_test, y_test, model))\n",
        "      print(f\"Métriques epoch {e + 1} : acc {accuracies[-1]:.4f}, \"\n",
        "            f\"val_acc {val_accuracies[-1]:.4f}, \"\n",
        "            f\"loss {losses[-1]:.4f}, \"\n",
        "            f\"val_loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "  x_ticks = numpy.arange(0, epochs + 1, evaluate_every)\n",
        "\n",
        "  plt.plot(x_ticks, losses, label=\"Train\")\n",
        "  plt.plot(x_ticks, val_losses, label=\"Val\")\n",
        "  plt.title(\"Fonction de perte pendant l'entrainement\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  plt.plot(x_ticks, accuracies, label=\"Train\")\n",
        "  plt.plot(x_ticks, val_accuracies, label=\"Val\")\n",
        "  plt.title(\"Accuracy pendant l'entrainement\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  return accuracies, val_accuracies, losses, val_losses\n",
        "\n",
        "\n",
        "_ = train(SimpleNN(), epochs=100, learning_rate=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kzt5Yn5OGkt"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itWcAjQ2OFbX"
      },
      "source": [
        "def train(model: typing.Any,\n",
        "          X_train: tf.Tensor = X_train,\n",
        "          y_train: tf.Tensor = y_train,\n",
        "          X_test: tf.Tensor = X_test,\n",
        "          y_test: tf.Tensor = y_test,\n",
        "          epochs: int = 150,\n",
        "          batch_size: int = 4000,\n",
        "          learning_rate: float = 0.001,\n",
        "          evaluate_every: int = 10,\n",
        "          loss_function = categorical_crossentropy\n",
        "         ) -> typing.Tuple[typing.List[float], ...]:\n",
        "  accuracies = [accuracy(y_train, model(X_train))]\n",
        "  val_accuracies = [accuracy(y_test, model(X_test))]\n",
        "  losses = [loss_function(X_train, y_train, model)]\n",
        "  val_losses = [loss_function(X_test, y_test, model)]\n",
        "\n",
        "  print(f\"Métriques initiales : acc {accuracies[-1]:.4f}, \"\n",
        "        f\"val_acc {val_accuracies[-1]:.4f}, \"\n",
        "        f\"loss {losses[-1]:.4f}, \"\n",
        "        f\"val_loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "  dataset_train = create_dataset(X_train, y_train)\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    for batch in dataset_train.as_numpy_iterator():\n",
        "\n",
        "      # Calcul de la perte avec enregistrement des opérations\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss = loss_function(batch[\"x\"], batch[\"y\"], model)\n",
        "\n",
        "      # Utilisation de l'enregistrement pour calculer automatiquement le gradient\n",
        "      gradients = tape.gradient(loss, model.variables)\n",
        "\n",
        "      # Parcours des variables une par une pour les mettre à jour en suivant la\n",
        "      # règle : nouvelle_valeur = ancienne_valeur - learning_rate * gradient\n",
        "      for gradient, variable in zip(gradients, model.trainable_variables):\n",
        "        variable.assign_sub(gradient * learning_rate)\n",
        "\n",
        "    # Calcul et affichage de la métrique d'évaluation sur l'ensemble de\n",
        "    # validation toutes les `evaluate_every` epochs\n",
        "    if (e + 1) % evaluate_every == 0:\n",
        "      accuracies.append(accuracy(y_train, model(X_train)))\n",
        "      losses.append(loss_function(X_train, y_train, model))\n",
        "      val_accuracies.append(accuracy(y_test, model(X_test)))\n",
        "      val_losses.append(loss_function(X_test, y_test, model))\n",
        "      print(f\"Métriques epoch {e + 1} : acc {accuracies[-1]:.4f}, \"\n",
        "            f\"val_acc {val_accuracies[-1]:.4f}, \"\n",
        "            f\"loss {losses[-1]:.4f}, \"\n",
        "            f\"val_loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "  x_ticks = numpy.arange(0, epochs + 1, evaluate_every)\n",
        "\n",
        "  plt.plot(x_ticks, losses, label=\"Train\")\n",
        "  plt.plot(x_ticks, val_losses, label=\"Val\")\n",
        "  plt.title(\"Fonction de perte pendant l'entrainement\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  plt.plot(x_ticks, accuracies, label=\"Train\")\n",
        "  plt.plot(x_ticks, val_accuracies, label=\"Val\")\n",
        "  plt.title(\"Accuracy pendant l'entrainement\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "  return accuracies, val_accuracies, losses, val_losses\n",
        "\n",
        "\n",
        "_ = train(SimpleNN(), epochs=100, learning_rate=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dZmS9V2DZ0F"
      },
      "source": [
        "## Passage à des réseaux profonds\n",
        "\n",
        "Pour passer à des réseaux profonds, on ajoute des matrices de poids et de biais pour chaque couche à notre fonction de création de poids, par exemple comme suit :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6l11YYHGsUQ"
      },
      "source": [
        "def create_mlp_weights(n_inputs: int,\n",
        "                       n_outputs: int,\n",
        "                       hidden_layer_sizes: typing.List[int] = [512],\n",
        "                      ) -> typing.List[typing.Dict[str, tf.Variable]]:\n",
        "  variables = []\n",
        "\n",
        "  # On garde en mémoire la dernière taille de sortie : ça sera la nouvelle\n",
        "  # taille d'entrée. Vaut n_inputs au départ\n",
        "  last_size = n_inputs\n",
        "\n",
        "  for i, hidden_layer_size in enumerate(hidden_layer_sizes, 1):\n",
        "    # On initialise aléatoirement une matrice de poids et un vecteur de biais\n",
        "    W = tf.random.normal((last_size, hidden_layer_size))\n",
        "    b = tf.zeros((hidden_layer_size,))\n",
        "\n",
        "    # On ajoute ces deux variables dans un dictionnaire puis dans la liste de\n",
        "    # nos variables\n",
        "    variables.append(dict(W=tf.Variable(W), b=tf.Variable(b)))\n",
        "\n",
        "    # On met à jour la dernière taille de sortie utilisée\n",
        "    last_size = hidden_layer_size\n",
        "\n",
        "  # La dernière couche est spéciale : elle fait toujours n_outputs en taille de\n",
        "  # sortie, on la traite donc à part\n",
        "  W = tf.random.normal((last_size, n_outputs))\n",
        "  b = tf.zeros((n_outputs,))\n",
        "  variables.append(dict(W=tf.Variable(W), b=tf.Variable(b)))\n",
        "  return variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x0Y0ryBELJ1"
      },
      "source": [
        "Pour ce qui est de la sortie du modèle, elle est maintenant calculée itérativement, couche par couche. Chaque couche prend en entrée le résultat de la couche précédente. Par simplicité, vous pourrez fixer vous même la fonction d'activation que vous utiliserez pour les couches cachées.\n",
        "\n",
        "*Complétez la fonction `MLP.__call__` pour calculer le résultat d'un réseau profond. Chaque variable est accessible grâce à `self.weights[layer_number][variable_name]`. Par exemple, pour accéder à la matrice `W` du premier layer : `self.weights[0][\"W\"]`.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQjlaOHGD3Kb"
      },
      "source": [
        "class MLP(tf.Module):\n",
        "  def __init__(self,\n",
        "               hidden_layer_sizes: typing.List[int] = [512],\n",
        "               n_inputs: int = 28 ** 2,\n",
        "               n_outputs: int = 10):\n",
        "    super().__init__()\n",
        "    self.weights = create_mlp_weights(n_inputs, n_outputs, hidden_layer_sizes)\n",
        "\n",
        "  def __call__(self, X: tf.Tensor):\n",
        "    # Votre code ici\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaLOndYYD8Tb"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnx-y5j9D0UF"
      },
      "source": [
        "class MLP(tf.Module):\n",
        "  def __init__(self,\n",
        "               hidden_layer_sizes: typing.List[int] = [512],\n",
        "               n_inputs: int = 28 ** 2,\n",
        "               n_outputs: int = 10):\n",
        "    super().__init__()\n",
        "    self.weights = create_mlp_weights(n_inputs, n_outputs, hidden_layer_sizes)\n",
        "\n",
        "  def __call__(self, X: tf.Tensor):\n",
        "    current = X\n",
        "    for layer_weights in self.weights:\n",
        "      # Calcul de la sortie de la couche de neurones non activée\n",
        "      current = current @ layer_weights[\"W\"] + layer_weights[\"b\"]\n",
        "      # Activation si on n'est pas dans la couche de sortie\n",
        "      if layer_weights is not self.weights[-1]:\n",
        "        current = tf.nn.tanh(current)\n",
        "    return tf.nn.log_softmax(current)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCfgkcIWE-6M"
      },
      "source": [
        "## Entraînement du réseau profond"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrSQl4e-FBe7"
      },
      "source": [
        "_ = train(MLP(), learning_rate=0.5, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI8iJHKmDEsC"
      },
      "source": [
        "*Que constatez-vous en entraînant un réseau avec une couche cachée de 512 neurones ?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyt6uU84Eq40"
      },
      "source": [
        "Votre réponse ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6_S6rkREt2o"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E6ZsSDkEvaD"
      },
      "source": [
        "Le réseau apprend moins bien que le réseau précédent qui n'avait pas de couche cachée : il a une loss quasi-parfaite sur l'ensemble d'entraînement mais commet quasiment 50% d'erreurs en plus sur la validation. C'est un cas de sur-apprentissage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oCK8I2ODHvq"
      },
      "source": [
        "## Régularisation\n",
        "\n",
        "La régularisation est un bon moyen de lutter contre le phénomène que l'on constate en entrainant notre réseau avec une couche cachée.\n",
        "\n",
        "Pour la mettre en place, on rajoute un terme à la fonction de perte qui pénalise les valeurs importantes des poids.\n",
        "\n",
        "*Modifiez la fonction ci-dessous pour calculer la pénalité L2. Elle est égale à la somme des carrés des paramètres.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PClIMOqt8p9"
      },
      "source": [
        "def regularized_categorical_crossentropy(X: tf.Tensor,\n",
        "                                         y: tf.Tensor,\n",
        "                                         model: typing.Any,\n",
        "                                         weights_norm_lambda: float = 0.003\n",
        "                                        ) -> tf.Tensor:\n",
        "  forward = model(X)\n",
        "  cross_entropy = keras.losses.sparse_categorical_crossentropy(y,\n",
        "                                                               forward,\n",
        "                                                               from_logits=True)\n",
        "  weights_norm = tf.constant(0, dtype=\"float32\")\n",
        "  # Votre code ici\n",
        "  return tf.reduce_mean(cross_entropy) + weights_norm_lambda * weights_norm\n",
        "\n",
        "\n",
        "train(MLP(),\n",
        "      epochs=100,\n",
        "      learning_rate=0.3,\n",
        "      loss_function=regularized_categorical_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_QA09cGNhCP"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vutn6sRONiBa"
      },
      "source": [
        "def regularized_categorical_crossentropy(X: tf.Tensor,\n",
        "                                         y: tf.Tensor,\n",
        "                                         model: typing.Any,\n",
        "                                         weights_norm_lambda: float = 0.003\n",
        "                                        ) -> tf.Tensor:\n",
        "  forward = model(X)\n",
        "  cross_entropy = keras.losses.sparse_categorical_crossentropy(y,\n",
        "                                                               forward,\n",
        "                                                               from_logits=True)\n",
        "  weights_norm = tf.constant(0, dtype=\"float32\")\n",
        "  for variable in model.variables:\n",
        "    weights_norm += tf.reduce_sum(tf.math.square(variable))\n",
        "  return tf.reduce_mean(cross_entropy) + weights_norm_lambda * weights_norm\n",
        "\n",
        "\n",
        "train(MLP(),\n",
        "      epochs=100,\n",
        "      learning_rate=0.3,\n",
        "      loss_function=regularized_categorical_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5A6WEOJHgZq"
      },
      "source": [
        "## Recherche d'hyper-paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpEVwVdhMtzd"
      },
      "source": [
        "Nous allons maintenant procéder à la recherche d'hyper-paramètres. Utilisez `random.sample` pour échantillonner le learning rate et le nombre d'unités cachées et renvoyez une liste des paramètres et d'une métrique au choix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muLzgxe0Hfta"
      },
      "source": [
        "def search_hyperparameters(X_train: tf.Tensor = X_train,\n",
        "                           y_train: tf.Tensor = y_train,\n",
        "                           X_test: tf.Tensor = X_test,\n",
        "                           y_test: tf.Tensor = y_test,\n",
        "                           learning_rates: typing.List[int] = [0.1, 0.5, 1],\n",
        "                           ns_hidden_units: typing.List[int]  = range(32, 513, 32),\n",
        "                           n_iters: int = 5\n",
        "                          ):\n",
        "  params = []\n",
        "  val_accs = []\n",
        "  for _ in range(n_iters):\n",
        "    pass\n",
        "  return params, val_accs\n",
        "\n",
        "\n",
        "search_hyperparameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgaETskNHCL"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KYFC783NIFz"
      },
      "source": [
        "def search_hyperparameters(X_train: tf.Tensor = X_train,\n",
        "                           y_train: tf.Tensor = y_train,\n",
        "                           X_test: tf.Tensor = X_test,\n",
        "                           y_test: tf.Tensor = y_test,\n",
        "                           learning_rates: typing.List[int] = [0.1, 0.5, 1],\n",
        "                           ns_hidden_units: typing.List[int]  = range(32, 513, 32),\n",
        "                           n_iters: int = 5\n",
        "                          ):\n",
        "  params = []\n",
        "  val_accs = []\n",
        "  for _ in range(n_iters):\n",
        "    learning_rate = random.sample(learning_rates, 1)[0]\n",
        "    n_hidden_units = random.sample(ns_hidden_units, 1)[0]\n",
        "    mlp = MLP(hidden_layer_sizes=[n_hidden_units])\n",
        "    _, val_acc_epochs, _, _ = train(mlp, X_train, y_train, X_test, y_test,\n",
        "                                    learning_rate=learning_rate, epochs=50)\n",
        "    val_accs.append(val_acc_epochs[-1])\n",
        "    params.append(dict(learning_rate=learning_rate,\n",
        "                       n_hidden_units=n_hidden_units))\n",
        "  return params, val_accs\n",
        "\n",
        "\n",
        "search_hyperparameters()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}